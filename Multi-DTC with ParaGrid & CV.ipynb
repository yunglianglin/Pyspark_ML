{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('DTC').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD & DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_df = spark.read.format(\"csv\")\\\n",
    "         .option(\"header\", \"true\")\\\n",
    "         .option(\"delimiter\", \"\\t\")\\\n",
    "         .load(\"hdfs://mycluster/user/oracle/dtc/dtc_data/covtype.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textfile will be transformed into dataframe using spark.read.format\n",
    "type(row_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = sc.textFile(\"hdfs://mycluster/user/oracle/dtc/dtc_data/covtype.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform textfile into RDD\n",
    "type(rawData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform RDD into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581012\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "lines = rawData.map(lambda x:x.split(\",\"))\n",
    "#581012 records in total \n",
    "print(lines.count())\n",
    "\n",
    "#each record contains 55 columns\n",
    "fieldnum = len(lines.first())\n",
    "print(fieldnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "#create a list of columns \n",
    "#StructField(column_name, column_type, True-> cannot be null)\n",
    "#create schema using StructType\n",
    "schema = StructType(\n",
    "                    [StructField(\"f\"+str(i), StringType(), True)\n",
    "                     for i in range(fieldnum)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(lines, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the datatype from StringType to DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "df = df.select([col(column).cast(\"double\").alias(column)\n",
    "                for column in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- f0: double (nullable = true)\n",
      " |-- f1: double (nullable = true)\n",
      " |-- f2: double (nullable = true)\n",
      " |-- f3: double (nullable = true)\n",
      " |-- f4: double (nullable = true)\n",
      " |-- f5: double (nullable = true)\n",
      " |-- f6: double (nullable = true)\n",
      " |-- f7: double (nullable = true)\n",
      " |-- f8: double (nullable = true)\n",
      " |-- f9: double (nullable = true)\n",
      " |-- f10: double (nullable = true)\n",
      " |-- f11: double (nullable = true)\n",
      " |-- f12: double (nullable = true)\n",
      " |-- f13: double (nullable = true)\n",
      " |-- f14: double (nullable = true)\n",
      " |-- f15: double (nullable = true)\n",
      " |-- f16: double (nullable = true)\n",
      " |-- f17: double (nullable = true)\n",
      " |-- f18: double (nullable = true)\n",
      " |-- f19: double (nullable = true)\n",
      " |-- f20: double (nullable = true)\n",
      " |-- f21: double (nullable = true)\n",
      " |-- f22: double (nullable = true)\n",
      " |-- f23: double (nullable = true)\n",
      " |-- f24: double (nullable = true)\n",
      " |-- f25: double (nullable = true)\n",
      " |-- f26: double (nullable = true)\n",
      " |-- f27: double (nullable = true)\n",
      " |-- f28: double (nullable = true)\n",
      " |-- f29: double (nullable = true)\n",
      " |-- f30: double (nullable = true)\n",
      " |-- f31: double (nullable = true)\n",
      " |-- f32: double (nullable = true)\n",
      " |-- f33: double (nullable = true)\n",
      " |-- f34: double (nullable = true)\n",
      " |-- f35: double (nullable = true)\n",
      " |-- f36: double (nullable = true)\n",
      " |-- f37: double (nullable = true)\n",
      " |-- f38: double (nullable = true)\n",
      " |-- f39: double (nullable = true)\n",
      " |-- f40: double (nullable = true)\n",
      " |-- f41: double (nullable = true)\n",
      " |-- f42: double (nullable = true)\n",
      " |-- f43: double (nullable = true)\n",
      " |-- f44: double (nullable = true)\n",
      " |-- f45: double (nullable = true)\n",
      " |-- f46: double (nullable = true)\n",
      " |-- f47: double (nullable = true)\n",
      " |-- f48: double (nullable = true)\n",
      " |-- f49: double (nullable = true)\n",
      " |-- f50: double (nullable = true)\n",
      " |-- f51: double (nullable = true)\n",
      " |-- f52: double (nullable = true)\n",
      " |-- f53: double (nullable = true)\n",
      " |-- f54: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+-----+---+-----+-----+-----+-----+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|    f0|  f1| f2|   f3| f4|   f5|   f6|   f7|   f8|    f9|f10|f11|f12|f13|f14|f15|f16|f17|f18|f19|f20|f21|f22|f23|f24|f25|f26|f27|f28|f29|f30|f31|f32|f33|f34|f35|f36|f37|f38|f39|f40|f41|f42|f43|f44|f45|f46|f47|f48|f49|f50|f51|f52|f53|f54|\n",
      "+------+----+---+-----+---+-----+-----+-----+-----+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|2596.0|51.0|3.0|258.0|0.0|510.0|221.0|232.0|148.0|6279.0|1.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|1.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|0.0|5.0|\n",
      "+------+----+---+-----+---+-----+-----+-----+-----+------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let f0~f53 columns be features \n",
    "featuresCol=df.columns[:54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.withColumn -> create the \"label\" column\n",
    "#-1 -> the value of label should be in the 0-6 range \n",
    "df=df.withColumn(\"label\", df[\"f54\"]-1).drop(\"f54\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training set & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(406288, 174724)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count(), test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#combine featuresCol into one vector\n",
    "assembler = VectorAssembler(\n",
    "            inputCols=featuresCol,\n",
    "            outputCol=\"features\")\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "#to build models with different parameter set -> 2(impurity)*3(depth)*3(bins)=18 models\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(dt.impurity, [\"gini\",\"entropy\"])\\\n",
    "    .addGrid(dt.maxDepth, [10, 15, 25])\\\n",
    "    .addGrid(dt.maxBins, [30,40,50])\\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "                rawPredictionCol=\"rawPrediction\",\n",
    "                labelCol=\"label\",\n",
    "                metricName=\"areaUnderROC\")\n",
    "\n",
    "#numFolds=3 ->two-third of training set is sub-training set, \n",
    "#one-third of training set is validation set\n",
    "#each model will be validated for three times beacuse numfolds=3\n",
    "#thus, the number of iteration will be 18*3=54 times\n",
    "cv=CrossValidator(estimator=dt, evaluator=evaluator,\n",
    "                  estimatorParamMaps=paramGrid, numFolds=3)\n",
    "\n",
    "cv_pipeline = Pipeline(stages=[assembler, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_43079bb5ca450aca7aaa, CrossValidator_40c4bb881d1897f77d01]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_pipeline.getStages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pipelineModel=cv_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pipelineModel.stages[1].bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=cv_pipelineModel.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select( 'rawPrediction','probability', 'label', 'prediction').take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=predictions.withColumnRenamed(\"f0\",\"latitude\")\\\n",
    "                    .withColumnRenamed(\"f1\", \"direction\")\\\n",
    "                    .withColumnRenamed(\"f2\", \"slope\")\\\n",
    "                    .withColumnRenamed(\"f3\", \"vertical distance\")\\\n",
    "                    .withColumnRenamed(\"f4\", \"horizontal distance\")\\\n",
    "                    .withColumnRenamed(\"f5\", \"shadow\")\n",
    "result.select(\"latitude\", \"direction\", \"slope\", \"vertical distance\", \"horizontal distance\", \"shadow\", \"label\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluator.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load pipeline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_pipelineModel.save(\"hdfs://mycluster/user/oracle/dtc/dtc_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "reloaded_cv_model= Pipeline.load(\"hdfs://mycluster/user/oracle/dtc/dtc_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_cv_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
